{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f19c46891e5c4d",
   "metadata": {},
   "source": [
    "<h1 id=\"activations\" >Activation Functions</h1>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f221a40e223666",
   "metadata": {},
   "source": [
    "## <h2 id=\"softmax-activation-function\">Softmax</h2>\n",
    "\n",
    "### Problem Domain\n",
    "\n",
    "Multiclass classification problems are very common in machine learning. For example, classifiers used for object recognition often need to recognize thousands of distinct categories of objects. Natural language models that try to predict the next word in a sentence may have to choose among tens of thousands of possible words. For this kind of prediction, we need the network to output a categorical distribution that is, if there are $d$ possible answers, we need $d$ output nodes that represent probabilities summing to 1.\n",
    "\n",
    "### Solution\n",
    "\n",
    "To achieve this, we use a **softmax** layer, which outputs a vector of $d$ values given a vector of input values **$in = <{\\text{in}_{1}, \\ldots, \\text{in}_{d}}>$**. The th element of that output vector is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax(in)}_{k} &= \\frac{e^{in_{k}}}{\\sum_{k^{j}=1}^{d} e^{in_{k}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $in_{k}$ is the k-th element of the input vector.\n",
    "- $e$ is the base of the natural logarithm (Euler's number).\n",
    "- $e^{in_{k}}$ is the exponential of the k-th element of the input vector.\n",
    "- $\\sum_{k^{j}=1}^{d} e^{in_{k}}$ is the sum of the exponential of all elements in the input vector.\n",
    "- $d$ is the dimensionality of the input vector.\n",
    "- The output is a probability distribution over the $d$ classes.\n",
    "\n",
    "#### Key Points\n",
    "- The softmax is clean and differentiable, unlike the `max` function.\n",
    "- softmax units propagate multiclass information.\n",
    "- The softmax function is differentiable, which allows us to use it in backpropagation.\n",
    "- The softmax function is used in the output layer of neural networks for multiclass classification problems.\n",
    "- It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. \n",
    "- The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes."
   ]
  },
  {
   "cell_type": "code",
   "id": "935b0735dac542d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:01.587858Z",
     "start_time": "2024-08-19T06:36:01.583045Z"
    }
   },
   "source": [
    "# Example Usage\n",
    "import numpy as np\n",
    "from functions.activations import Softmax\n",
    "\n",
    "# If the vector inputs are given by:\n",
    "__in = np.array([5, 2, 0, -2])\n",
    "\n",
    "# Then the outputs are:\n",
    "exps = np.exp(__in - np.max(__in))    # To avoid overflow\n",
    "outputs = exps / np.sum(exps, axis=0, keepdims=True)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['0.945683', '0.047083', '0.006372', '0.000862']"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.945683, 0.047083, 0.006372, 0.000862]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:02.155950Z",
     "start_time": "2024-08-19T06:36:02.149815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functions.activations import Softmax\n",
    "\n",
    "__in = np.array([5, 2, 0, -2])\n",
    "softmax = Softmax()\n",
    "outputs = softmax(__in)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['0.945683', '0.047083', '0.006372', '0.000862']"
   ],
   "id": "be86f5ffd82c0dd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.945683, 0.047083, 0.006372, 0.000862]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "7f0b15a6",
   "metadata": {},
   "source": [
    "\n",
    "## ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "\n",
    "In deep learning models, especially in the layers of neural networks, non-linear activation functions are required to capture complex patterns. ReLU is one of the most popular activation functions due to its simplicity and effectiveness in practice.\n",
    "\n",
    "### Solution\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{y} &= \\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $\\max(a, b)$ returns the maximum of $a$ and $b$. In this case, it returns $0$ if $x$ is negative, and $x$ otherwise.\n",
    "- $y$ is the output of the activation function.\n",
    "\n",
    "This means that it outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "\n",
    "### Key Points\n",
    "- **Non-linear**: The ReLU function introduces non-linearity to the model, allowing it to learn complex patterns.\n",
    "- **Sparse Activation**: For any given input, some neurons will be inactive (outputting zero), which can make the network more efficient.\n",
    "- The ReLU function is a piecewise linear function that outputs the input directly if it is positive, and zero otherwise.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "88cfc5d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:03.134797Z",
     "start_time": "2024-08-19T06:36:03.128997Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = relu(x)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: [0 0 1 2]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1.0, 2.0]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "740bb3fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:03.570054Z",
     "start_time": "2024-08-19T06:36:03.564849Z"
    }
   },
   "source": [
    "\n",
    "from functions.activations import ReLU\n",
    "\n",
    "relu = ReLU()\n",
    "outputs = relu(np.array([-1, 0, 1, 2]))\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: [0 0 1 2]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1.0, 2.0]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "011df5cf",
   "metadata": {},
   "source": [
    "\n",
    "## Sigmoid Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "The Sigmoid function is often used in binary classification problems or as the activation function for the output layer of a neural network when the output needs to be in the range (0, 1), such as in probability predictions.\n",
    "\n",
    "### Solution\n",
    "The Sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{y} &= \\text{Sigmoid}(x) &= \\frac{1}{1 + e^{-x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $e$ is the base of the natural logarithm (Euler's number).\n",
    "- $e^{-x}$ is the exponential of the negative input.\n",
    "- $y$ is the output of the activation function.\n",
    "\n",
    "This function maps any real-valued number into the range (0, 1).\n",
    "\n",
    "### Key Points\n",
    "- **Smooth Gradient**: The Sigmoid function has a smooth gradient, which makes it suitable for backpropagation.\n",
    "- **Output Range**: The output is always between 0 and 1, making it ideal for probability estimation.\n",
    "- **Vanishing Gradient**: For extreme input values, the gradient becomes very small, which can slow down learning in deep networks.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aabf8a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:04.519306Z",
     "start_time": "2024-08-19T06:36:04.513781Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = sigmoid(x)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: [0.26894142 0.5 0.73105858 0.88079708]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.268941, 0.5, 0.731059, 0.880797]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "0185d8cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:05.146947Z",
     "start_time": "2024-08-19T06:36:05.141257Z"
    }
   },
   "source": [
    "\n",
    "from functions.activations import Sigmoid\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "outputs = sigmoid(np.array([-1, 0, 1, 2]))\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['0.268941', '0.500000', '0.731059', '0.880797']\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.268941, 0.5, 0.731059, 0.880797]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "cfee721b",
   "metadata": {},
   "source": [
    "\n",
    "## Tanh Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "The Tanh (Hyperbolic Tangent) function is commonly used in neural networks, particularly for hidden layers. Unlike the Sigmoid function, the Tanh function outputs values in the range (-1, 1), which can make learning more efficient in practice.\n",
    "\n",
    "### Solution\n",
    "The Tanh function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{y} &= \\tanh(x) &= \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $e$ is the base of the natural logarithm (Euler's number).\n",
    "- $e^{x}$ is the exponential of the input.\n",
    "- $y$ is the output of the activation function.\n",
    "\n",
    "**Note**: Tanh maps any real-valued number into the range (-1, 1). Tanh \n",
    "is a scaled and shifted version of the sigmoid, as $\\tanh(x) = 2\\sigma(2x) - 1$.\n",
    "\n",
    "### Key Points\n",
    "- **Zero-centered**: Unlike the Sigmoid function, Tanh is zero-centered, meaning that negative inputs will map strongly negative, zero inputs will map near zero, and positive inputs will map strongly positive.\n",
    "- **Smooth Gradient**: The Tanh function has a smooth gradient, which is advantageous for gradient-based optimization methods.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63968a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:06.249649Z",
     "start_time": "2024-08-19T06:36:06.244569Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.tanh(x) - 1) / (np.tanh(x) + 1)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = tanh(x)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['-0.761594', '0.000000', '0.761594', '0.964028']"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.389056, -1.0, -0.135335, -0.018316]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "11ef8d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:07.079018Z",
     "start_time": "2024-08-19T06:36:07.072953Z"
    }
   },
   "source": [
    "\n",
    "from functions.activations import Tanh\n",
    "\n",
    "tanh = Tanh()\n",
    "outputs = tanh(np.array([-1, 0, 1, 2]))\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['-0.761594', '0.000000', '0.761594', '0.964028']\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.761594, 0.0, 0.761594, 0.964028]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "7b8a4a93",
   "metadata": {},
   "source": [
    "\n",
    "## Leaky ReLU Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "A potential issue with the ReLU activation function is the \"dying ReLU\" problem, where neurons can become inactive and only output zero. Leaky ReLU is a variation that attempts to fix this by allowing a small, non-zero gradient when the input is negative.\n",
    "\n",
    "### Solution\n",
    "The Leaky ReLU function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases} \n",
    "      x & x \\geq 0 \\\\\n",
    "      \\alpha x & x < 0 \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $\\alpha$ is a small positive constant.\n",
    "- The function outputs the input directly if it is positive, and $\\alpha x$ \n",
    "if it is negative.\n",
    "- This small slope for negative inputs helps to keep the gradient alive and \n",
    "prevent neurons from dying.\n",
    "- The Leaky ReLU function is a piecewise linear function that outputs the \n",
    "input directly if it is positive, and a small fraction of the input otherwise.\n",
    "Where $( \\alpha )$ is a small positive constant (e.g., 0.01).\n",
    "\n",
    "### Key Points\n",
    "- **Fixes \"Dying ReLU\"**: Leaky ReLU introduces a small slope for negative inputs, which helps to keep the gradient alive even for negative inputs.\n",
    "- **Simple and Effective**: This modification is simple to implement and has been shown to be effective in practice.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a05eb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:09.668706Z",
     "start_time": "2024-08-19T06:36:09.661689Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = leaky_relu(x)\n",
    "print(outputs)\n",
    "# Output: [-0.01  0.    1.    2.  ]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "f84da258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T06:36:10.428499Z",
     "start_time": "2024-08-19T06:36:10.423225Z"
    }
   },
   "source": [
    "\n",
    "from functions.activations import LeakyReLU\n",
    "\n",
    "leaky_relu = LeakyReLU()\n",
    "outputs = leaky_relu(np.array([-1, 0, 1, 2]))\n",
    "print(outputs)\n",
    "# Output: [-0.01  0.    1.    2.  ]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
