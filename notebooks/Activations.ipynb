{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f19c46891e5c4d",
   "metadata": {},
   "source": [
    "<h1 id=\"activations\" >Activation Functions</h1>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T02:59:43.600600Z",
     "start_time": "2024-09-08T02:59:43.570368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.utils.logger import getLogger\n",
    "\n"
   ],
   "id": "745cba31786f942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables.\n",
      "Directory already exists: logs\n",
      "Directory already exists: datasets\n",
      "Directory already exists: models\n",
      "Loaded Environment Variables: \n",
      "{\n",
      "  \"LOG_LEVEL\": \"INFO\",\n",
      "  \"PYTHONENV\": \"development\",\n",
      "  \"PYTHONPATH\": \".\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "81f221a40e223666",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "### Problem Domain\n",
    "\n",
    "Multiclass classification problems are very common in machine learning. For example, classifiers used for object recognition often need to recognize thousands of distinct categories of objects. Natural language models that try to predict the next word in a sentence may have to choose among tens of thousands of possible words. For this kind of prediction, we need the network to output a categorical distribution that is, if there are $d$ possible answers, we need $d$ output nodes that represent probabilities summing to 1.\n",
    "\n",
    "### Solution\n",
    "\n",
    "To achieve this, we use a **softmax** layer, which outputs a vector of $d$ values given a vector of input values **$in = <{\\text{in}_{1}, \\ldots, \\text{in}_{d}}>$**. The th element of that output vector is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{softmax(in)}_{k} &= \\frac{e^{in_{k}}}{\\sum_{k^{j}=1}^{d} e^{in_{k}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "\n",
    "- $in_{k}$ is the k-th element of the input vector.\n",
    "- $e$ is the base of the natural logarithm (Euler's number).\n",
    "- $e^{in_{k}}$ is the exponential of the k-th element of the input vector.\n",
    "- $\\sum_{k^{j}=1}^{d} e^{in_{k}}$ is the sum of the exponential of all elements in the input vector.\n",
    "- $d$ is the dimensionality of the input vector.\n",
    "- The output is a probability distribution over the $d$ classes.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- The softmax is clean and differentiable, unlike the `max` function.\n",
    "- softmax units propagate multiclass information.\n",
    "- The softmax function is differentiable, which allows us to use it in backpropagation.\n",
    "- The softmax function is used in the output layer of neural networks for multiclass classification problems.\n",
    "- It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. \n",
    "- The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T02:59:44.551895Z",
     "start_time": "2024-09-08T02:59:43.614714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage\n",
    "import numpy as np\n",
    "\n",
    "from src.functions.normalize import normalize_data\n",
    "\n",
    "# If the vector inputs are given by:\n",
    "__output = np.array([5, 2, 0, -2])\n",
    "\n",
    "# TODO: From Scratch lets calculate the softmax function\n",
    "# Create the mathematical expression e\n",
    "E = 2.718281828459045 # Euler's number, alternatively use math.e\n",
    "\n",
    "# Calculate the exponential of each element in the input vector\n",
    "exp_values = [E ** i for i in __output]\n",
    "\n",
    "# Not normalized exponential values\n",
    "normalize_base = sum(exp_values)\n",
    "\n",
    "# Normalized exponential values\n",
    "normalize_values = [i / normalize_base for i in exp_values]\n",
    "print(f\"\"\"Normalized values: \\n{[float(f'{a:5f}') for a in normalize_values]}\"\"\")\n",
    "\n",
    "# Sum of normalized values\n",
    "sum_norm_values = sum(normalize_values)\n",
    "print(f\"Sum of normalized values: {sum_norm_values}\")\n",
    "\n",
    "\n",
    "# Calculate the exponential of each element in the input vector\n",
    "exp_values = np.exp(__output)\n",
    "\n",
    "# Normalized exponential values, summed row wise along axis 0\n",
    "normalize_values = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "print(f\"\"\"Normalized values: \\n{[float(f'{a:5f}') for a in normalize_values]}\"\"\")\n",
    "\n",
    "# Sum of normalized values\n",
    "sum_norm_values = sum(normalize_values)\n",
    "print(f\"Sum of normalized values: {sum_norm_values}\")\n",
    "# Output: ['0.945683', '0.047083', '0.006372', '0.000862']"
   ],
   "id": "259d34be87c8c59c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values: \n",
      "[0.945683, 0.047083, 0.006372, 0.000862]\n",
      "Sum of normalized values: 1.0000000000000002\n",
      "Normalized values: \n",
      "[0.945683, 0.047083, 0.006372, 0.000862]\n",
      "Sum of normalized values: 1.0000000000000002\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T02:59:44.816087Z",
     "start_time": "2024-09-08T02:59:44.811221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Now we numpy to calculate the softmax function\n",
    "# Calculate the exponential of each element in the input vector\n",
    "exps = np.exp(__output - np.max(__output))    # To avoid overflow\n",
    "outputs = exps / np.sum(exps, axis=0, keepdims=True)\n",
    "print(f\"\"\"Normalized values: \\n{[float(f'{a:5f}') for a in outputs]}\"\"\")\n",
    "sum_norm_values = sum(outputs)\n",
    "print(f\"Sum of normalized values: {sum_norm_values}\")\n",
    "# Output: ['0.945683', '0.047083', '0.006372', '0.000862']"
   ],
   "id": "a82046a498f266f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values: \n",
      "[0.945683, 0.047083, 0.006372, 0.000862]\n",
      "Sum of normalized values: 1.0000000000000002\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T02:59:44.831294Z",
     "start_time": "2024-09-08T02:59:44.824809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.functions.activation import Softmax\n",
    "import numpy as np\n",
    "\n",
    "__output = np.array([4.8, 1.21, 2.385])\n",
    "softmax = Softmax()\n",
    "outputs = softmax(__output)\n",
    "print(outputs)\n",
    "print(sum(outputs))\n",
    "# Output: [0.89528266 0.02470831 0.08000903]\n"
   ],
   "id": "be86f5ffd82c0dd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89528266 0.02470831 0.08000903]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T02:59:44.847466Z",
     "start_time": "2024-09-08T02:59:44.841662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example Usage\n",
    "output = softmax(np.array([-2,-1,0]))\n",
    "print([float(f'{a:5f}') for a in output])\n",
    "# Output: [0.090031, 0.244728, 0.665241]\n",
    "output = softmax(np.array([1,2,3]))\n",
    "print([float(f'{a:5f}') for a in output])\n",
    "# Output: [0.090031, 0.244728, 0.665241]\n",
    "output = softmax(np.array([0.5, 1.0, 1.5]))\n",
    "print([float(f'{a:5f}') for a in output])\n",
    "# Output: [0.186324, 0.307196, 0.50648]"
   ],
   "id": "4ad1ff24e96b15bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.090031, 0.244728, 0.665241]\n",
      "[0.090031, 0.244728, 0.665241]\n",
      "[0.186324, 0.307196, 0.50648]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T02:59:45.310484Z",
     "start_time": "2024-09-08T02:59:44.857983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we create a dense layer with 3 neurons with 2 inputs each and 2 dense layers; the first layer has 3 neurons with 2 inputs each and the second layer has 3 neurons with 3 inputs each.\n",
    "from src.layer.dense import Dense\n",
    "from src.utils.datasets import create_spiral_dataset\n",
    "from src.functions.activation import Softmax, ReLU\n",
    "\n",
    "# Initialize activation function\n",
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "# Create a spiral dataset\n",
    "X, y = create_spiral_dataset(100, 3)\n",
    "print(f\"Inputs: {X.shape}\")\n",
    "print(f\"Y is a spiral dataset: {y.shape}\")\n",
    "# Create a dense layer with 3 neurons with 2 inputs each\n",
    "dense = Dense(2, 3)\n",
    "\n",
    "# Lets do the forward pass\n",
    "dense.forward(X)\n",
    "print(f\"Weights Layer 1: {dense.weights.shape}\")\n",
    "print(f\"Biases Layer 1: {dense.biases.shape}\")\n",
    "print(f\"Output Layer 1: {dense.output.shape}\")\n",
    "\n",
    "# TODO: These final outputs are also our “confidence scores.” The higher the confidence score, the more confident the model is that the input belongs to that class.\n",
    "# Run the activation function ReLU\n",
    "predictions = softmax(dense.output)\n",
    "print(f\"Predictions: {predictions.shape}\")\n",
    "\n",
    "# Calculate the loss\n",
    "avg_loss, loss = dense.loss(np.array([y]), predictions)\n",
    "print(f\"Loss: {avg_loss}\")\n",
    "\n",
    "# Run ArgMax to get the predicted class\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f\"Predicted Class: {predicted_class.shape}\")"
   ],
   "id": "446d8f899c8f6b8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: (300, 2)\n",
      "Y is a spiral dataset: (300,)\n",
      "Weights Layer 1: (2, 3)\n",
      "Biases Layer 1: (1, 3)\n",
      "Output Layer 1: (300, 3)\n",
      "Predictions: (300, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes of true labels and predicted probabilities must be compatible.\n                True Labels Shape: \n(3, 300)\n                Predicted Probabilities Shape: \n(1, 300)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredictions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpredictions\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Calculate the loss\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m avg_loss, loss \u001B[38;5;241m=\u001B[39m \u001B[43mdense\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43my\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Run ArgMax to get the predicted class\u001B[39;00m\n",
      "File \u001B[0;32m~/SynologyDrive/Repos/NeuralNetworkFromScratch/src/layer/dense.py:148\u001B[0m, in \u001B[0;36mDense.loss\u001B[0;34m(self, y_pred, y_true)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;124;03mCalculate the loss of the layer\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;124;03m:param y_pred: (N_Neurons): The predicted values\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;124;03m:param y_true: (N_Neurons): The true values\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;124;03m:return: (float): The loss value\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(y_pred, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(y_true, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m--> 148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(y_pred, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(y_true, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cross_entropy_loss(y_true, y_pred)\n",
      "File \u001B[0;32m~/SynologyDrive/Repos/NeuralNetworkFromScratch/src/functions/loss.py:68\u001B[0m, in \u001B[0;36mcross_entropy_loss\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m         log\u001B[38;5;241m.\u001B[39merror(e)\n\u001B[0;32m---> 68\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Compute the cross-entropy loss using vectorized operations\u001B[39;00m\n\u001B[1;32m     71\u001B[0m epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-15\u001B[39m  \u001B[38;5;66;03m# Small constant to avoid log(0)\u001B[39;00m\n",
      "File \u001B[0;32m~/SynologyDrive/Repos/NeuralNetworkFromScratch/src/functions/loss.py:63\u001B[0m, in \u001B[0;36mcross_entropy_loss\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     61\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mtranspose(y_true)\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y_true\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m y_pred\u001B[38;5;241m.\u001B[39mshape:\n\u001B[0;32m---> 63\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mShapes of true labels and predicted probabilities must be compatible.\u001B[39m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;124m        True Labels Shape: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00my_true\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;124m        Predicted Probabilities Shape: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00my_pred\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     log\u001B[38;5;241m.\u001B[39merror(e)\n",
      "\u001B[0;31mValueError\u001B[0m: Shapes of true labels and predicted probabilities must be compatible.\n                True Labels Shape: \n(3, 300)\n                Predicted Probabilities Shape: \n(1, 300)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "7f0b15a6",
   "metadata": {},
   "source": [
    "\n",
    "## ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "\n",
    "In deep learning models, especially in the layers of neural networks, non-linear activation functions are required to capture complex patterns. ReLU is one of the most popular activation functions due to its simplicity and effectiveness in practice.\n",
    "\n",
    "### Solution\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{y} &= \\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $\\max(a, b)$ returns the maximum of $a$ and $b$. In this case, it returns $0$ if $x$ is negative, and $x$ otherwise.\n",
    "- $y$ is the output of the activation function.\n",
    "\n",
    "This means that it outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "\n",
    "### Key Points\n",
    "- **Non-linear**: The ReLU function introduces non-linearity to the model, allowing it to learn complex patterns.\n",
    "- **Sparse Activation**: For any given input, some neurons will be inactive (outputting zero), which can make the network more efficient.\n",
    "- The ReLU function is a piecewise linear function that outputs the input directly if it is positive, and zero otherwise.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "88cfc5d0",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = relu(x)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: [0 0 1 2]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "740bb3fd",
   "metadata": {},
   "source": [
    "\n",
    "from src.functions.activation import ReLU\n",
    "\n",
    "relu = ReLU()\n",
    "outputs = relu(np.array([-1, 0, 1, 2]))\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: [0 0 1 2]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "011df5cf",
   "metadata": {},
   "source": [
    "\n",
    "## Sigmoid Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "The Sigmoid function is often used in binary classification problems or as the activation function for the output layer of a neural network when the output needs to be in the range (0, 1), such as in probability predictions.\n",
    "\n",
    "### Solution\n",
    "The Sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{y} &= \\text{Sigmoid}(x) &= \\frac{1}{1 + e^{-x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $e$ is the base of the natural logarithm (Euler's number).\n",
    "- $e^{-x}$ is the exponential of the negative input.\n",
    "- $y$ is the output of the activation function.\n",
    "\n",
    "This function maps any real-valued number into the range (0, 1).\n",
    "\n",
    "### Key Points\n",
    "- **Smooth Gradient**: The Sigmoid function has a smooth gradient, which makes it suitable for backpropagation.\n",
    "- **Output Range**: The output is always between 0 and 1, making it ideal for probability estimation.\n",
    "- **Vanishing Gradient**: For extreme input values, the gradient becomes very small, which can slow down learning in deep networks.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aabf8a7b",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = sigmoid(x)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: [0.26894142 0.5 0.73105858 0.88079708]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0185d8cc",
   "metadata": {},
   "source": [
    "\n",
    "from src.functions.activation import Sigmoid\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "outputs = sigmoid(np.array([-1, 0, 1, 2]))\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['0.268941', '0.500000', '0.731059', '0.880797']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfee721b",
   "metadata": {},
   "source": [
    "\n",
    "## Tanh Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "The Tanh (Hyperbolic Tangent) function is commonly used in neural networks, particularly for hidden layers. Unlike the Sigmoid function, the Tanh function outputs values in the range (-1, 1), which can make learning more efficient in practice.\n",
    "\n",
    "### Solution\n",
    "The Tanh function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{y} &= \\tanh(x) &= \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $e$ is the base of the natural logarithm (Euler's number).\n",
    "- $e^{x}$ is the exponential of the input.\n",
    "- $y$ is the output of the activation function.\n",
    "\n",
    "**Note**: Tanh maps any real-valued number into the range (-1, 1). Tanh \n",
    "is a scaled and shifted version of the sigmoid, as $\\tanh(x) = 2\\sigma(2x) - 1$.\n",
    "\n",
    "### Key Points\n",
    "- **Zero-centered**: Unlike the Sigmoid function, Tanh is zero-centered, meaning that negative inputs will map strongly negative, zero inputs will map near zero, and positive inputs will map strongly positive.\n",
    "- **Smooth Gradient**: The Tanh function has a smooth gradient, which is advantageous for gradient-based optimization methods.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63968a25",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.tanh(x) - 1) / (np.tanh(x) + 1)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = tanh(x)\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['-0.761594', '0.000000', '0.761594', '0.964028']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11ef8d58",
   "metadata": {},
   "source": [
    "\n",
    "from src.functions.activation import Tanh\n",
    "\n",
    "tanh = Tanh()\n",
    "outputs = tanh(np.array([-1, 0, 1, 2]))\n",
    "print([float(f'{a:5f}') for a in outputs])\n",
    "# Output: ['-0.761594', '0.000000', '0.761594', '0.964028']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b8a4a93",
   "metadata": {},
   "source": [
    "\n",
    "## Leaky ReLU Activation Function\n",
    "\n",
    "### Problem Domain\n",
    "A potential issue with the ReLU activation function is the \"dying ReLU\" problem, where neurons can become inactive and only output zero. Leaky ReLU is a variation that attempts to fix this by allowing a small, non-zero gradient when the input is negative.\n",
    "\n",
    "### Solution\n",
    "The Leaky ReLU function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases} \n",
    "      x & x \\geq 0 \\\\\n",
    "      \\alpha x & x < 0 \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Where:\n",
    "- $x$ is the input to the function.\n",
    "- $\\alpha$ is a small positive constant.\n",
    "- The function outputs the input directly if it is positive, and $\\alpha x$ \n",
    "if it is negative.\n",
    "- This small slope for negative inputs helps to keep the gradient alive and \n",
    "prevent neurons from dying.\n",
    "- The Leaky ReLU function is a piecewise linear function that outputs the \n",
    "input directly if it is positive, and a small fraction of the input otherwise.\n",
    "Where $( \\alpha )$ is a small positive constant (e.g., 0.01).\n",
    "\n",
    "### Key Points\n",
    "- **Fixes \"Dying ReLU\"**: Leaky ReLU introduces a small slope for negative inputs, which helps to keep the gradient alive even for negative inputs.\n",
    "- **Simple and Effective**: This modification is simple to implement and has been shown to be effective in practice.\n",
    "\n",
    "### Manual Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a05eb9e",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1, 2])\n",
    "outputs = leaky_relu(x)\n",
    "print(outputs)\n",
    "# Output: [-0.01  0.    1.    2.  ]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f84da258",
   "metadata": {},
   "source": [
    "\n",
    "from src.functions.activation import LeakyReLU\n",
    "\n",
    "leaky_relu = LeakyReLU()\n",
    "outputs = leaky_relu(np.array([-1, 0, 1, 2]))\n",
    "print(outputs)\n",
    "# Output: [-0.01  0.    1.    2.  ]\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
