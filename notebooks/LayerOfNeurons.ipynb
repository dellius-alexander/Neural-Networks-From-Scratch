{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neural Networks\n",
    "\n",
    "## A mathematical model of a neural network is a collection of neurons that are connected in layers\n",
    "\n",
    "### A neural network is a collection of neurons that are connected in layers.\n",
    "- A neural network has an input layer, hidden layers, and an output layer.\n",
    "- The input layer is the first layer of the neural network.\n",
    "    - The input layer has neurons that take the input to the neural network.\n",
    "    - **Note**: The input to the neural network is the data that the network is trained on. \\\n",
    "    Not the data that the network is predicting. The input sits at the beginning of the neurons \\\n",
    "    dendrites and aggregates the input signals via dot product to the nucleus to which we \\\n",
    "    apply an activation function, add the bias and pass the output to the axon terminals of \\\n",
    "    other neurons.\n",
    "- The hidden layers are the layers between the input and output layers.\n",
    "- The output layer is the last layer of the neural network.\n",
    "    - The output layer has neurons that produce the output of the neural network.\n",
    "    - The output of the neural network is the prediction of the network.\n",
    "\n",
    "---\n",
    "\n",
    "### A neural networkâ€™s forward pass:\n",
    "- The forward pass is the process of calculating the output of a neural network given an input.\n",
    "- The output of the neural network is calculated using the weights and biases of the neurons in the network.\n",
    "- The output of the neural network is the prediction of the network.\n",
    "- The prediction of the network is the output of the last layer of neurons.\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L = -\\sum_{l=1}^{N} y_l \\log \\left( \\frac{\\exp\\left(\\sum_{i=1}^{n_2}\\left(\\forall_{j=1}^{n_2} \\max\\left(0,\\sum_{i=1}^{n_1}\\left(\\forall_{j=1}^{n_1} \\max\\left(0,\\sum_{i=1}^{n_0} X_i w_{1,i,j} + b_{1,j}\\right)\\right)w_{2,i,j} + b_{2,j}\\right)\\right)w_{3,i,j} + b_{3,j}\\right)}{\\sum_{k=1}^{n_3} \\exp\\left(\\sum_{i=1}^{n_2}\\left(\\forall_{j=1}^{n_2} \\max\\left(0,\\sum_{i=1}^{n_1}\\left(\\forall_{j=1}^{n_1} \\max\\left(0,\\sum_{i=1}^{n_0} X_i w_{1,i,j} + b_{1,j}\\right)\\right)w_{2,i,j} + b_{2,k}\\right)\\right)w_{3,i,k} + b_{3,k}\\right)}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "---"
   ],
   "id": "d63c06f41b1fed64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The Forward Pass can be represented as a series of matrix multiplications ",
   "id": "ef3e52a2bcef9ccf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from src.functions.activation import Sigmoid\n",
    "\n",
    "\n",
    "# create the input data\n",
    "X = np.array([[1.0, 2.0, 3.0, 2.5],\n",
    "                [2.0, 5.0, -1.0, 2.0],\n",
    "                [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# create the Expected output data\n",
    "y = np.array([[0, 1, 0],\n",
    "                [1, 0, 1],\n",
    "                [0, 1, 0]])\n",
    "\n",
    "# create the weights\n",
    "w1 = np.array([[0.2, 0.8, -0.5, 1.0],\n",
    "                [0.5, -0.91, 0.26, -0.5],\n",
    "                [-0.26, -0.27, 0.17, 0.87]])\n",
    "w2 = np.array([[0.1, -0.14, 0.5],\n",
    "                [-0.5, 0.12, -0.33],\n",
    "                [-0.44, 0.73, -0.13]])\n",
    "w3 = np.array([[-0.1, -0.14, -0.5],\n",
    "                [0.5, 0.12, -0.33],\n",
    "                [-0.44, 0.73, -0.13]])\n",
    "\n",
    "# create the biases\n",
    "b1 = np.array([2.0, 3.0, 0.5])\n",
    "b2 = np.array([-1.0, 2.0, -0.5])\n",
    "b3 = np.array([2.0, 3.0, 0.5])\n",
    "\n",
    "# create the activation function\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# calculate the output/loss of the neural network\n",
    "loss = -np.log(  # cross-entropy loss\n",
    "    np.sum(  # sum over the output neurons\n",
    "        y * np.exp(  # element-wise multiplication with the exponential of the output\n",
    "            np.dot(  # dot product of the output\n",
    "                np.maximum(  # ReLU activation\n",
    "                    0,  # ReLU\n",
    "                    np.dot(  # dot product of the hidden layer\n",
    "                        np.maximum(  # ReLU activation\n",
    "                            0,  # ReLU\n",
    "                            np.dot(  # dot product of the input layer\n",
    "                                X,  # input data\n",
    "                                w1.T  # transpose of the weights\n",
    "                            ) + b1 # add the bias\n",
    "                        ),  # ReLU\n",
    "                        w2.T  # transpose of the weights\n",
    "                    ) + b2  # add the bias\n",
    "                ),  # ReLU\n",
    "                w3.T  # transpose of the weights\n",
    "            ) + b3  # add the bias\n",
    "        ) /  # divide by the sum of the exponential of the output\n",
    "        np.sum(  # sum over the output neurons\n",
    "            np.exp(  # exponential of the output\n",
    "                np.dot(  # dot product of the output\n",
    "                    np.maximum(  # ReLU activation\n",
    "                        0,  # ReLU\n",
    "                        np.dot(  # dot product of the hidden layer\n",
    "                            np.maximum(  # ReLU activation\n",
    "                                0,  # ReLU\n",
    "                                np.dot(  # dot product of the input layer\n",
    "                                    X,  # input data\n",
    "                                    w1.T  # transpose of the weights\n",
    "                                ) + b1  # add the bias\n",
    "                            ),  # ReLU\n",
    "                            w2.T  # transpose of the weights\n",
    "                        ) + b2  # add the bias\n",
    "                    ),  # ReLU\n",
    "                    w3.T  # transpose of the weights\n",
    "                ) + b3  # add the bias\n",
    "            ),  # exponential of the output\n",
    "        axis=1, # sum over the output neurons\n",
    "        keepdims=True  # keep the dimensions of the output\n",
    "        ) # sum over the output neurons\n",
    "    )  # sum over the output neurons\n",
    ")  # cross-entropy loss\n",
    "\n",
    "# verify the accuracy of the loss\n",
    "# assert loss.shape == (3, 3), \"The shape of the loss is incorrect\"\n",
    "\n",
    "# print the loss\n",
    "print(loss)"
   ],
   "id": "c52aa918f101e9a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Layer of Neurons\n",
    "\n",
    "### A layer of neurons is a collection of neurons that take the same number of inputs and produce the same number of outputs.\n",
    "\n",
    "#### The output of each neuron is calculated as follows:\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{``Predictions``} & = \\text{Activation Function}(\\text{weights} \\cdot \\text{inputs} + \\text{Bias}) \\\\\n",
    "\\text{``Weighted Sum of Inputs w/ Bias``} & = \\sigma(\\sum_{i=1}^{n} w_i \\cdot x_i + b) \\\\\n",
    "\\text{``Weighted Sum of Inputs w/ Bias``} & = \\sigma(w_1 \\cdot x_1 + w_2 \\cdot x_2 + . . . + w_n \\cdot x_n + b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### The Predictions are the output of the layer of neurons.\n",
    "- The weights are the weights of the neurons in the layer.\n",
    "- The inputs are the inputs to the layer.\n",
    "- The bias is the bias of the neurons in the layer.\n",
    "- The activation function is the activation function of the neurons in the layer.\n",
    "- The weighted sum of inputs w/ bias is the weighted sum of the inputs to the layer plus the bias.\n",
    "- The weighted sum of inputs w/ bias is the dot product of the weights and inputs plus the bias.\n",
    "\n",
    "#### The output of each neuron is calculated as follows:\n",
    "- The weighted sum of inputs and bias is calculated.\n",
    "- The activation function is applied to the weighted sum of inputs and bias.\n",
    "- The result is the prediction of the neuron.\n",
    "- The predictions of all the neurons in the layer are returned as a list.\n",
    "- The output of the layer of neurons is the list of predictions.\n",
    "\n",
    "#### The output of the layer of neurons is a list of predictions, one for each neuron in the layer.\n",
    "\n",
    "### Using Dot Product\n",
    "\n",
    "#### Calculate the weighted sum of inputs and add the bias\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Weighted Sum w/ Bias} & = \\sum_{i=1}^{n} w_i \\cdot x_i + b \\\\\n",
    "\\text{Weighted Sum w/ Bias} & = w_1 \\cdot x_1 + w_2 \\cdot x_2 + . . . + w_n \\cdot x_n + b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "# calculate the weighted sum of inputs and add the bias for each neuron\n",
    "output = [\n",
    "# Neuron 1: \n",
    "inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias,\n",
    "]\n",
    "\n",
    "predictions = ActivationFunction(output)\n",
    "```\n",
    "\n",
    "In the context of binary classification using a sigmoid activation function, a prediction close to \"1\" typically indicates a positive class, while a prediction close to \"0\" indicates a negative class. Whether \"1\" or \"0\" is considered good or bad depends on the true label of the data point:\n",
    "\n",
    "- If the true label is \"1\" (positive class), a prediction close to \"1\" is good, and a prediction close to \"0\" is bad.\n",
    "- If the true label is \"0\" (negative class), a prediction close to \"0\" is good, and a prediction close to \"1\" is bad.\n",
    "\n"
   ],
   "id": "830d3fc99896b802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:25:37.286255Z",
     "start_time": "2024-08-19T20:25:37.274379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Layer of Neurons Example \n",
    "import numpy as np\n",
    "from src.functions.activation import Sigmoid\n",
    "from src.encoder.label import encode as encode_labels\n",
    "\n",
    "# initialize the activation function\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# convert the words to sums of Unicode values\n",
    "input_words = np.array(['Cat', 'Dog', 'Rabbit', 'Horse'])\n",
    "encoded_inputs = encode_labels(input_words)\n",
    "print(f\"Encoded Inputs: \\n{encoded_inputs}\")\n",
    "\n",
    "# initialize seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "\n",
    "# TODO: Our weights define the number of neurons in the layer. This layer has 4 neurons with 4 inputs each to match your input data. In the 'np.random.rand(4, 4)' function, the first argument is the number of neurons in the layer, and the second argument is the number of inputs to each neuron.\n",
    "# initialize random weights and biases\n",
    "weights = np.random.rand(4, 1)\n",
    "print(f\"Weights: \\n{weights}\")\n",
    "\n",
    "# initialize random bias\n",
    "# This layer has 4 neurons, so we need 4 biases.\n",
    "# The bias is a 4x1 matrix. Meaning we have 4 biases for the 4 neurons in the layer, in the shape of a 4x1 matrix and 1 input per neuron.\n",
    "bias  = np.random.rand(4, 1)\n",
    "print(f\"Bias: \\n{bias}\")\n",
    "\n",
    "# convert the words to sums of Unicode values\n",
    "labels = np.array(['Cat', 'Dog', 'Rabbit', 'Horse'])\n",
    "encoded_labels = encode_labels(labels)\n",
    "print(f\"Encoded Labels: \\n{encoded_labels}\")\n",
    "\n",
    "# get the weighted sums of the inputs and add the bias\n",
    "outputs = np.dot(weights, encoded_inputs)  + bias\n",
    "print(f\"Outputs: \\n{outputs}\")\n",
    "\n",
    "# apply the activation function and get the predictions\n",
    "# The prediction is a sort of transformation of the output of the neurons in the layer.\n",
    "predictions = sigmoid(outputs)\n",
    "print(f\"Predictions: \\n{predictions}\")\n",
    "\n",
    "# Error\n",
    "error_rate = 1 - predictions\n",
    "print(f\"Error Ratio: \\n{error_rate}\")"
   ],
   "id": "ebe52e88d18caffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Inputs: \n",
      "[[104.       104.666664 104.666664 109.      ]]\n",
      "Weights: \n",
      "[[0.54340494]\n",
      " [0.27836939]\n",
      " [0.42451759]\n",
      " [0.84477613]]\n",
      "Bias: \n",
      "[[0.00471886]\n",
      " [0.12156912]\n",
      " [0.67074908]\n",
      " [0.82585276]]\n",
      "Encoded Labels: \n",
      "[[104.       104.666664 104.666664 109.      ]]\n",
      "Outputs: \n",
      "[[56.5188328  56.88110138 56.88110138 59.23585751]\n",
      " [29.07198517 29.25756405 29.25756405 30.4638321 ]\n",
      " [44.82057852 45.10358917 45.10358917 46.94316648]\n",
      " [88.68257052 89.24575246 89.24575246 92.90645118]]\n",
      "Predictions: \n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "Error Ratio: \n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.36699549e-13 1.96509475e-13 1.96509475e-13 5.88418203e-14]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:57:09.229687Z",
     "start_time": "2024-08-19T19:57:09.219173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.encoder.label import encode as encode_labels\n",
    "\n",
    "# Test the label encoder\n",
    "labels = [\"cat\", \"dog\", \"fish\", \"elephant\", \"lion\", \"tiger\", \"bear\"]\n",
    "encoded_labels = encode_labels(labels)\n",
    "print(f\"Encoded labels: \\n{encoded_labels}\\n\")\n",
    "print(f\"Data type: \\n{encoded_labels.dtype}\\n\")\n",
    "print(f\"Shape: \\n{encoded_labels.shape}\\n\")\n",
    "print(f\"Size: \\n{encoded_labels.size}\\n\")\n",
    "print(f\"Number of dimensions: \\n{encoded_labels.ndim}\\n\")\n",
    "print(f\"Item size: \\n{encoded_labels.itemsize}\\n\")\n",
    "print(f\"Total bytes: \\n{encoded_labels.nbytes}\\n\")\n",
    "print(f\"Strides: \\n{encoded_labels.strides}\\n\")\n",
    "print(f\"Flags: \\n{encoded_labels.flags}\\n\")\n",
    "print(f\"Ctypes: \\n{encoded_labels.ctypes}\\n\")\n",
    "print(f\"Base: \\n{encoded_labels.base}\\n\")\n",
    "print(f\"Data: \\n{encoded_labels.data}\\n\")\n",
    "print(f\"Transpose: \\n{encoded_labels.T}\\n\")\n",
    "print(f\"Real part: \\n{encoded_labels.real}\\n\")\n",
    "print(f\"Imaginary part: \\n{encoded_labels.imag}\\n\")\n",
    "print(f\"Flat: \\n{encoded_labels.flat}\\n\")\n",
    "print(f\"Item: \\n{encoded_labels.item}\\n\")\n",
    "print(f\"List: \\n{encoded_labels.tolist()}\\n\")\n",
    "print(f\"Bytes: {encoded_labels.tobytes()}\\n\")"
   ],
   "id": "d0d21742f1e33526",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: \n",
      "[[104.       104.666664 106.5      106.125    108.5      107.8\n",
      "  102.5     ]]\n",
      "\n",
      "Data type: \n",
      "float32\n",
      "\n",
      "Shape: \n",
      "(1, 7)\n",
      "\n",
      "Size: \n",
      "7\n",
      "\n",
      "Number of dimensions: \n",
      "2\n",
      "\n",
      "Item size: \n",
      "4\n",
      "\n",
      "Total bytes: \n",
      "28\n",
      "\n",
      "Strides: \n",
      "(28, 4)\n",
      "\n",
      "Flags: \n",
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : True\n",
      "  OWNDATA : True\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "\n",
      "\n",
      "Ctypes: \n",
      "<numpy._core._internal._ctypes object at 0x1088246b0>\n",
      "\n",
      "Base: \n",
      "None\n",
      "\n",
      "Data: \n",
      "<memory at 0x1088cd7d0>\n",
      "\n",
      "Transpose: \n",
      "[[104.      ]\n",
      " [104.666664]\n",
      " [106.5     ]\n",
      " [106.125   ]\n",
      " [108.5     ]\n",
      " [107.8     ]\n",
      " [102.5     ]]\n",
      "\n",
      "Real part: \n",
      "[[104.       104.666664 106.5      106.125    108.5      107.8\n",
      "  102.5     ]]\n",
      "\n",
      "Imaginary part: \n",
      "[[0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Flat: \n",
      "<numpy.flatiter object at 0x7fa8c0f42e00>\n",
      "\n",
      "Item: \n",
      "<built-in method item of numpy.ndarray object at 0x106e46310>\n",
      "\n",
      "List: \n",
      "[[104.0, 104.66666412353516, 106.5, 106.125, 108.5, 107.80000305175781, 102.5]]\n",
      "\n",
      "Bytes: b'\\x00\\x00\\xd0BUU\\xd1B\\x00\\x00\\xd5B\\x00@\\xd4B\\x00\\x00\\xd9B\\x9a\\x99\\xd7B\\x00\\x00\\xcdB'\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Graph the error on a line plot\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=labels, y=error_rate, mode='lines+markers'))\n",
    "fig.update_layout(title='Error Ratio', xaxis_title='Labels', yaxis_title='Error')\n",
    "fig.show()"
   ],
   "id": "a131bfe5a9629d2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "inputs=[1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1] \n",
    "weights2 = [0.5, -0.91, 0.26, -0.5] \n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2 \n",
    "bias2 = 3 \n",
    "bias3 = 0.5\n",
    "\n",
    "# calculate the weighted sum of inputs and add the bias for three neurons.\n",
    "outputs = np.array([\n",
    "    # Neuron 1:\n",
    "    inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "    # Neuron 2: \n",
    "    inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "    # Neuron 3: \n",
    "    inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3\n",
    "    ])\n",
    "print(outputs)\n",
    "\n",
    "# apply the activation function and get the predictions\n",
    "predictions = sigmoid(outputs)\n",
    "print(predictions)\n",
    "\n",
    "# [4.8, 1.21, 2.385]\n",
    "# [0.99194602, 0.77015115, 0.9158585]\n",
    "\n",
    "# Error ratio is the difference between the prediction and the actual value\n",
    "error_ratio = 1 - predictions\n",
    "print(error_ratio)"
   ],
   "id": "daafd12053f1a968",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
